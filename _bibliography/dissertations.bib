---
---
@article{DBLP:journals/corr/abs-2211-10280,
 abbr={CoRR},
  author    = {Mauro Dalle Lucca Tosi and
               Vinu E. Venugopal and
               Martin Theobald},
  title     = {TensAIR: Online Learning from Data Streams via Asynchronous Iterative
               Routing},
  journal   = {CoRR},
  volume    = {abs/2211.10280},
  year      = {2022},
  url       = {https://doi.org/10.48550/arXiv.2211.10280},
  doi       = {10.48550/arXiv.2211.10280},
  eprinttype = {arXiv},
  eprint    = {2211.10280},
  timestamp = {Thu, 24 Nov 2022 15:52:33 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2211-10280.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  abstract = {Online learning (OL) from data streams is an emerging area of research that encompasses numerous challenges from stream processing, machine learning, and networking. Recent extensions of stream-processing platforms, such as Apache Kafka and Flink, already provide basic extensions for the training of neural networks in a stream-processing pipeline. However, these extensions are not scalable and flexible enough for many real-world use-cases, since they do not integrate the neural-network libraries as a first-class citizen into their architectures. In this paper, we present TensAIR, which provides an end-to-end dataflow engine for OL from data streams via a protocol to which we refer as asynchronous iterative routing. TensAIR supports the common dataflow operators, such as Map, Reduce, Join, and has been augmented by the data-parallel OL functions train and predict. These belong to the new Model operator, in which an initial TensorFlow model (either freshly initialized or pre-trained) is replicated among multiple decentralized worker nodes. Our decentralized architecture allows TensAIR to efficiently shard incoming data batches across the distributed model replicas, which in turn trigger the model updates via asynchronous stochastic gradient descent. We empirically demonstrate that TensAIR achieves a nearly linear scale-out in terms of (1) the number of worker nodes deployed in the network, and (2) the throughput at which the data batches arrive at the dataflow operators. We exemplify the versatility of TensAIR by investigating both sparse (Word2Vec) and dense (CIFAR-10) use-cases, for which we are able to demonstrate very significant performance improvements in comparison to Kafka, Flink, and Horovod. We also demonstrate the magnitude of these improvements by depicting the possibility of real-time concept drift adaptation of a sentiment analysis model trained over a Twitter stream.}
}

@article{VENUGOPAL202277,
  abbr={JPDC22},
  title = {Targeting a light-weight and multi-channel approach for distributed stream processing},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {167},
  pages = {77-96},
  year = {2022},
  issn = {0743-7315},
  doi = {https://doi.org/10.1016/j.jpdc.2022.04.022},
  url = {https://www.sciencedirect.com/science/article/pii/S0743731522001022},
  author = {Vinu Ellampallil Venugopal and Martin Theobald and Damien Tassetti and Samira Chaychi and Amal Tawakuli},
  keywords = {Distributed stream processing systems, Dataflow programming, Asynchronous iterative routing, Stateful windowed operators},
  abstract = {Processing high-throughput data-streams has become a major challenge in areas such as real-time event monitoring, complex dataflow processing, and big data analytics. While there has been tremendous progress in distributed stream processing systems in the past few years, the high-throughput and low-latency (a.k.a. high sustainable-throughput) requirement of modern applications is pushing the limits of traditional data processing infrastructures. This paper introduces a new distributed stream processing engine (DSPE), called Asynchronous Iterative Routing (or simply “AIR”), which implements a light-weight, dynamic sharding protocol. AIR expedites direct and asynchronous communication among all the worker nodes via a channel-like communication protocol on top of the Message Passing Interface (MPI), thereby completely avoiding the need for a dedicated driver node. The system adopts a new progress-tracking protocol, called hew-meld, which has been experimentally observed to show a low processing latency on our asynchronous master-less architecture when compared to the conventional low-watermark technique. The current version of AIR is also equipped with two fault tolerance and recovery strategies namely checkpointing & rollback and replication. With its unique design, AIR scales out particularly well to multi-core HPC architectures; specifically, we deployed it on clusters with up to 16 nodes and 448 cores (thus reaching a peak of 435.3 million events and 55.14 GB of data processed per second), which we found to significantly outperform existing DSPEs.}
}

@inproceedings{DBLP:conf/comad/TosiVT22,
abbr={CODS-COMAD22},
  author    = {Mauro Dalle Lucca Tosi and Vinu Ellampallil Venugopal and Martin Theobald},
  title     = {Convergence time analysis of Asynchronous Distributed Artificial Neural Networks},
  booktitle = {International Conference on Data Science {\&} Management of Data},
  pages     = {314--315},
  publisher = {{ACM}},
  year      = {2022},
  url       = {https://doi.org/10.1145/3493700.3493758},
  doi       = {10.1145/3493700.3493758},
  timestamp = {Thu, 23 Jun 2022 19:58:58 +0200},
  biburl    = {https://dblp.org/rec/conf/comad/TosiVT22.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DBLP:conf/bnaic/Venugopal021,
abbr={BNAIC21},
  author    = {Vinu Ellampallil Venugopal and
               P. Sreenivasa Kumar},
  title     = {Verbalizing but Not Just Verbatim Translations of Ontology Axioms},
  booktitle = {33rd Benelux Conference on Artificial Intelligence},
  series    = {Communications in Computer and Information Science},
  volume    = {1530},
  pages     = {170--186},
  publisher = {Springer},
  year      = {2021},
  url       = {https://doi.org/10.1007/978-3-030-93842-0\_10},
  doi       = {10.1007/978-3-030-93842-0\_10},
  timestamp = {Wed, 19 Jan 2022 14:35:08 +0100},
  biburl    = {https://dblp.org/rec/conf/bnaic/Venugopal021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
  }































